{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f135a74e",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "小鱼儿老师：课程15-DQN-捕食者游戏 https://www.bilibili.com/video/BV1nZ4y1X7bk/?p=3&spm_id_from=pageDriver&vd_source=eb79911dba31f65406585397357ca033\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "47aeb71a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "from matplotlib import style\n",
    "style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cf7bee51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 方格类：可实例化为玩家、食物、敌人\n",
    "class Cube:\n",
    "    def __init__(self, size):\n",
    "        self.size = size\n",
    "        self.x = np.random.randint(0, self.size)\n",
    "        self.y = np.random.randint(0, self.size)\n",
    "        \n",
    "    def __str__(self):\n",
    "        return f'{self.x},{self.y}'\n",
    "    \n",
    "    def __sub__(self, other):\n",
    "        return (self.x-other.x, self.y-other.y)\n",
    "    \n",
    "    def __eq__(self, other):\n",
    "        return (self - other == (0, 0))\n",
    "\n",
    "    # 1:向左或向下，2：不动，3：向右或向上\n",
    "    def action(self, choise):\n",
    "        if choise == 0:\n",
    "            self.move(x=2, y=2)\n",
    "        elif choise == 1:\n",
    "            self.move(x=3, y=2)\n",
    "        elif choise == 2:\n",
    "            self.move(x=2, y=3)\n",
    "        elif choise == 3:\n",
    "            self.move(x=2, y=1)\n",
    "        elif choise == 4:\n",
    "            self.move(x=1, y=2)\n",
    "        elif choise == 5:\n",
    "            self.move(x=3, y=1)\n",
    "        elif choise == 6:\n",
    "            self.move(x=3, y=3)\n",
    "        elif choise == 7:\n",
    "            self.move(x=1, y=3)\n",
    "        elif choise == 8:\n",
    "            self.move(x=1, y=1)  \n",
    "    \n",
    "    def move(self, x=False, y=False):\n",
    "        if not x:\n",
    "            self.x = self.x + np.random.randint(-1, 2)\n",
    "        else:\n",
    "            self.x = self.x + x - 2\n",
    "        if not y:\n",
    "            self.y = self.y + np.random.randint(-1, 2) \n",
    "        else:\n",
    "            self.y = self.y + y -2\n",
    "               \n",
    "        #边界情况\n",
    "        if self.x < 0:\n",
    "            self.x = 0\n",
    "        if self.x >= self.size:\n",
    "            self.x = self.size-1\n",
    "        if self.y < 0:\n",
    "            self.y = 0\n",
    "        if self.y >= self.size:\n",
    "            self.y = self.size-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fefb7f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 环境类 \n",
    "class envCube:\n",
    "    SIZE = 10\n",
    "\n",
    "    OBSERVATION_SPACE_VALUES = (4, ) #(2*SIZE-1, 2*SIZE-1, 2*SIZE-1, 2*SIZE-1) # 观测值空间\n",
    "    ACTION_SPACE_VALUES = 9 # action操作数\n",
    "\n",
    "    IMAGE_SHAPE = (SIZE, SIZE, 3) # 画面大小\n",
    "    RETURN_IMAGE = False # 是否返回图像为observation\n",
    "\n",
    "    FOOD_REWARD = 25 #食物奖励\n",
    "    MOVE_PENALITY = -1 #移动惩罚\n",
    "    ENEMY_PENALITY = -300 #敌人惩罚\n",
    "\n",
    "    \n",
    "    BGR = { 1:(255, 0, 0), # blue\n",
    "            2:(0, 255, 0), # green\n",
    "            3:(0, 0, 255)} # red\n",
    "        \n",
    "    PLAYER_N = 1\n",
    "    FOOD_N = 2\n",
    "    ENEMY_N = 3\n",
    "\n",
    "    # 重置环境\n",
    "    def reset(self):\n",
    "        self.player = Cube(self.SIZE)\n",
    "        self.food = Cube(self.SIZE)\n",
    "        while (self.player == self.food):\n",
    "            self.food = Cube(self.SIZE)\n",
    "\n",
    "        self.enemy = Cube(self.SIZE)\n",
    "        while (self.player == self.enemy or self.food == self.enemy):\n",
    "            self.enemy = Cube(self.SIZE)\n",
    "        \n",
    "        if self.RETURN_IMAGE:\n",
    "            observation = np.array(self.get_image())\n",
    "        else:\n",
    "            observation = (self.player - self.food) + (self.player - self.enemy)\n",
    "\n",
    "        # 局数重置\n",
    "        self.episode_step = 0\n",
    "\n",
    "        return observation\n",
    "    \n",
    "    '''\n",
    "    new_observation: 新状态\n",
    "    reward: 当前步骤的奖励\n",
    "    done: 游戏是否结束\n",
    "    '''\n",
    "    def step(self, action):\n",
    "        self.episode_step += 1\n",
    "\n",
    "        self.player.action(action)\n",
    "        self.food.move()\n",
    "        self.enemy.move()\n",
    "\n",
    "        if self.RETURN_IMAGE:\n",
    "            new_observation = np.array(self.get_image())\n",
    "        else:\n",
    "            new_observation = (self.player - self.food) + (self.player - self.enemy)\n",
    "\n",
    "        if (self.player == self.food):\n",
    "            reward = self.FOOD_REWARD\n",
    "        elif (self.player == self.enemy):\n",
    "            reward = self.ENEMY_PENALITY\n",
    "        else:\n",
    "            reward = self.MOVE_PENALITY\n",
    "        \n",
    "        done = False\n",
    "        if (self.player == self.food or self.player == self.enemy or self.episode_step >= 200):\n",
    "            done = True\n",
    "        info = {'prob': 1.0}\n",
    "        return new_observation, reward, done, info\n",
    "    \n",
    "\n",
    "    '''\n",
    "    获取画图数组\n",
    "    '''\n",
    "    def get_image(self):\n",
    "        env_image = np.zeros(self.IMAGE_SHAPE, dtype=np.uint8)\n",
    "        env_image[self.food.x][self.food.y] = self.BGR[self.FOOD_N]\n",
    "        env_image[self.player.x][self.player.y] = self.BGR[self.PLAYER_N]\n",
    "        env_image[self.enemy.x][self.enemy.y] = self.BGR[self.ENEMY_N]\n",
    "        image = Image.fromarray(env_image, 'RGB')\n",
    "        return image\n",
    "\n",
    "    '''\n",
    "    画图\n",
    "    '''\n",
    "    def render(self, mode='human'):\n",
    "        img = self.get_image()\n",
    "        img = img.resize((800, 800))\n",
    "        cv2.imshow('', np.array(img))\n",
    "        cv2.waitKey(1)\n",
    "\n",
    "    '''\n",
    "    q_table\n",
    "    '''\n",
    "    def init_q_table(self, q_table_name=None):\n",
    "        # 初始化Q——table\n",
    "        if q_table_name is None:\n",
    "            q_table = {}\n",
    "\n",
    "            for x1 in range(-self.SIZE+1, self.SIZE):\n",
    "                for y1 in range(-self.SIZE+1, self.SIZE):\n",
    "                    for x2 in range(-self.SIZE+1, self.SIZE):\n",
    "                        for y2 in range(-self.SIZE+1, self.SIZE):\n",
    "                            q_table[x1, y1, x2, y2] = [np.random.uniform(-5, 0) for i in range(self.ACTION_SPACE_VALUES)]                        \n",
    "        else:\n",
    "            with open(q_table_name, 'rb') as f:\n",
    "                q_table = pickle.load(f) \n",
    "        return q_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "611e6aaa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode #0, epsilon:0.6\n",
      "episode #3000, epsilon:0.32926722239570905\n",
      "mean reward:-180.657\n",
      "episode #6000, epsilon:0.18069483957364205\n",
      "mean reward:-157.46866666666668\n",
      "episode #9000, epsilon:0.09916147987941909\n",
      "mean reward:-129.065\n",
      "episode #12000, epsilon:0.054417708414240505\n",
      "mean reward:-111.47033333333333\n",
      "episode #15000, epsilon:0.029863279497827713\n",
      "mean reward:-98.514\n",
      "episode #18000, epsilon:0.016388331819794114\n",
      "mean reward:-90.45333333333333\n",
      "episode #21000, epsilon:0.00899356749667138\n",
      "mean reward:-83.074\n",
      "episode #24000, epsilon:0.00493547831509555\n",
      "mean reward:-78.41466666666666\n",
      "episode #27000, epsilon:0.0027084853933429503\n",
      "mean reward:-72.05366666666667\n",
      "episode #30000, epsilon:0.0014863591039423116\n",
      "mean reward:-68.44966666666667\n",
      "episode #33000, epsilon:0.0008156822227294347\n",
      "mean reward:-65.63766666666666\n",
      "episode #36000, epsilon:0.00044762903305946587\n",
      "mean reward:-68.509\n",
      "episode #39000, epsilon:0.0002456492806319455\n",
      "mean reward:-65.00266666666667\n",
      "episode #42000, epsilon:0.00013480709386197562\n",
      "mean reward:-58.683\n",
      "episode #45000, epsilon:7.397926225861729e-05\n",
      "mean reward:-57.24033333333333\n",
      "episode #48000, epsilon:4.0598243664631156e-05\n",
      "mean reward:-55.82233333333333\n",
      "episode #51000, epsilon:2.2279451542662007e-05\n",
      "mean reward:-55.84766666666667\n",
      "episode #54000, epsilon:1.2226488543253467e-05\n",
      "mean reward:-55.14066666666667\n",
      "episode #57000, epsilon:6.7096365371500425e-06\n",
      "mean reward:-56.278333333333336\n",
      "episode #60000, epsilon:3.682105643120264e-06\n",
      "mean reward:-54.078\n",
      "episode #63000, epsilon:2.0206611627963032e-06\n",
      "mean reward:-50.327333333333335\n",
      "episode #66000, epsilon:1.1088958141280404e-06\n",
      "mean reward:-52.722\n",
      "episode #69000, epsilon:6.085384077402802e-07\n",
      "mean reward:-52.785666666666664\n",
      "episode #72000, epsilon:3.3395291872958457e-07\n",
      "mean reward:-50.271\n",
      "episode #75000, epsilon:1.832662499350516e-07\n",
      "mean reward:-47.471\n",
      "episode #78000, epsilon:1.005726151249877e-07\n",
      "mean reward:-47.507\n",
      "episode #81000, epsilon:5.5192109385462725e-08\n",
      "mean reward:-46.88333333333333\n",
      "episode #84000, epsilon:3.0288254259185935e-08\n",
      "mean reward:-46.64\n",
      "episode #87000, epsilon:1.6621548918562064e-08\n",
      "mean reward:-45.071333333333335\n",
      "episode #90000, epsilon:9.121552073882217e-09\n",
      "mean reward:-46.096\n",
      "episode #93000, epsilon:5.005713525508368e-09\n",
      "mean reward:-45.717333333333336\n",
      "episode #96000, epsilon:2.747028981087967e-09\n",
      "mean reward:-43.531333333333336\n",
      "episode #99000, epsilon:1.5075110040722586e-09\n",
      "mean reward:-46.024\n",
      "97001\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#train\n",
    "'''\n",
    "import gymnasium as gym\n",
    "env = gym.make(\"LunarLander-v2\", render_mode=\"human\")\n",
    "observation, info = env.reset(seed=42)\n",
    "for _ in range(1000):\n",
    "   action = env.action_space.sample()  # this is where you would insert your policy\n",
    "   observation, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "   if terminated or truncated:\n",
    "      observation, info = env.reset()\n",
    "\n",
    "env.close()\n",
    "'''\n",
    "# 初始化环境参数\n",
    "SIZE = 10\n",
    "EPISODES = 100000\n",
    "SHOW_EVERY = 3000\n",
    "\n",
    "epsilon = 0.6 #选择随机概率\n",
    "EPS_DECAY = 0.9998 #概率折扣\n",
    "\n",
    "DISCOUNT = 0.95 #折扣 gamma\n",
    "LEARNING_RATE = 0.3 #alpha\n",
    "\n",
    "q_table_name = f'Q_table\\qtable_1719578112.pickle'\n",
    "\n",
    "env = envCube()\n",
    "obs = env.reset()\n",
    "q_table = env.init_q_table()\n",
    "\n",
    "# train\n",
    "episode_rewards = [] # 奖励序列\n",
    "for episode in range(EPISODES): \n",
    "    done = False   \n",
    "\n",
    "    # 显示图像 \n",
    "    if(episode % SHOW_EVERY == 0):\n",
    "        print(f'episode #{episode}, epsilon:{epsilon}')\n",
    "        flag_show = False\n",
    "        if(episode >= SHOW_EVERY):\n",
    "            print(f'mean reward:{np.mean(episode_rewards[-SHOW_EVERY:])}')\n",
    "            flag_show = True\n",
    "    else:\n",
    "        flag_show = False\n",
    "    \n",
    "    episode_reward = 0\n",
    "    while not done:\n",
    "        if np.random.random() > epsilon:\n",
    "            action = np.argmax(q_table[obs]) #选择Q值最高动作\n",
    "        else:\n",
    "            action = np.random.randint(0, env.ACTION_SPACE_VALUES) #随机选择一个动作\n",
    "            \n",
    "        newObs, reward, done, info = env.step(action)\n",
    "    \n",
    "        # Update the Q_table----------------------\n",
    "        current_q = q_table[obs][action]    # 当前动作、状态对应Q_value\n",
    "        max_future_q = np.max(q_table[obs]) # 新状态最大Q_value\n",
    "\n",
    "        if (reward == env.FOOD_REWARD):\n",
    "            new_q = env.FOOD_REWARD\n",
    "        else:\n",
    "            new_q = (1 - LEARNING_RATE) * current_q + LEARNING_RATE * (reward + DISCOUNT + max_future_q) \n",
    "\n",
    "        q_table[obs][action] = new_q\n",
    "        obs = newObs\n",
    "        # ---------------------------------\n",
    "        \n",
    "        if flag_show:\n",
    "            env.render()\n",
    "  \n",
    "        episode_reward += reward\n",
    "        \n",
    "    if done:\n",
    "        obs = env.reset()       \n",
    "    episode_rewards.append(episode_reward)\n",
    "    epsilon *= EPS_DECAY\n",
    "    \n",
    "cv2.destroyAllWindows()\n",
    "#画曲线\n",
    "moving_avg = np.convolve(episode_rewards, np.ones((SHOW_EVERY,))/SHOW_EVERY, mode = 'valid')\n",
    "print(len(moving_avg))\n",
    "plt.plot([i for i in range(len(moving_avg))], moving_avg)\n",
    "plt.xlabel('episode #')\n",
    "plt.ylabel(f'mean {SHOW_EVERY} reward')\n",
    "plt.show()\n",
    "\n",
    "if not os.path.exists('Q_table'):\n",
    "    os.makedirs('Q_table')\n",
    "with open(f'Q_table\\qtable_pro_{int(time.time())}.pickle', 'wb') as f:\n",
    "    pickle.dump(q_table, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3f59b57f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(q_table, episodes, show_enable=True):\n",
    "    env = envCube()\n",
    "    \n",
    "    obs = env.reset()\n",
    "    operations = []\n",
    "    rewards = []\n",
    "    for episode in range(episodes):\n",
    "        done = False\n",
    "        operation = 0\n",
    "        episode_reward = 0\n",
    "\n",
    "        while not done:\n",
    "            action = np.argmax(q_table[obs])\n",
    "            newObs, reward, done, info = env.step(action)\n",
    "            obs = newObs\n",
    "            operation += 1\n",
    "            episode_reward += reward\n",
    "\n",
    "            if show_enable:\n",
    "                env.render()\n",
    "              \n",
    "        if done:\n",
    "            print(f'episode: #{episode} reward:{episode_reward}  The number of operation:{operation}')\n",
    "            obs = env.reset()\n",
    "            operations.append(operation)\n",
    "            rewards.append(episode_reward)\n",
    "\n",
    "        cv2.waitKey(5000)\n",
    "        cv2.destroyAllWindows()\n",
    "\n",
    "    print(f'mean operations:{np.mean(operations[:])} mean reward:{np.mean(rewards[:])}')\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3996f304",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: #0 reward:-40  The number of operation:66\n",
      "episode: #1 reward:-27  The number of operation:53\n",
      "episode: #2 reward:-12  The number of operation:38\n",
      "episode: #3 reward:-92  The number of operation:118\n",
      "episode: #4 reward:3  The number of operation:23\n",
      "episode: #5 reward:20  The number of operation:6\n",
      "episode: #6 reward:21  The number of operation:5\n",
      "episode: #7 reward:-9  The number of operation:35\n",
      "episode: #8 reward:-31  The number of operation:57\n",
      "episode: #9 reward:18  The number of operation:8\n",
      "episode: #10 reward:-93  The number of operation:119\n",
      "episode: #11 reward:20  The number of operation:6\n",
      "episode: #12 reward:5  The number of operation:21\n",
      "episode: #13 reward:-6  The number of operation:32\n",
      "episode: #14 reward:-348  The number of operation:49\n",
      "episode: #15 reward:-7  The number of operation:33\n",
      "episode: #16 reward:23  The number of operation:3\n",
      "episode: #17 reward:7  The number of operation:19\n",
      "episode: #18 reward:18  The number of operation:8\n",
      "episode: #19 reward:-132  The number of operation:158\n",
      "mean operations:42.85 mean reward:-33.1\n"
     ]
    }
   ],
   "source": [
    "with open('Q_table\\qtable_pro_1720684190.pickle', 'rb') as f:\n",
    "    q_table_save = pickle.load(f)\n",
    "test(q_table_save, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c5251c4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from rl.agents.dqn import DQNAgent\n",
    "from rl.policy import BoltzmannQPolicy\n",
    "from rl.memory import SequentialMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1673f589",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from rl.agents.dqn import DQNAgent\n",
    "from rl.policy import BoltzmannQPolicy\n",
    "from rl.memory import SequentialMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b3d46f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(status, nb_actions): \n",
    "    # Next, we build a very simple model.\n",
    "    model = Sequential()\n",
    "    model.add(Flatten(input_shape=(2,) + status)) # status: env.observation_space.shape\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    # 输出层\n",
    "    model.add(Dense(nb_actions, activation='linear')) # nb_actions: env.action_space.n\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fa00f093",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_agent(model, nb_actions):\n",
    "    # Finally, we configure and compile our agent. You can use every built-in Keras optimizer and\n",
    "    # even the metrics!\n",
    "    memory = SequentialMemory(limit=50000, window_length=2) # window_length 窗口值，应该与model输入层的（1,)对应，即输入样本数对应\n",
    "    policy = BoltzmannQPolicy() # 玻尔兹曼策略\n",
    "    dqn = DQNAgent(model=model, nb_actions=nb_actions, memory=memory, nb_steps_warmup=1000, \n",
    "                   target_model_update=1e-2, policy=policy)\n",
    "    dqn.compile(Adam(lr=1e-3), metrics=['mae'])\n",
    "    return dqn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee066026",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42190e3b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a388f348",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_2 (Flatten)          (None, 8)                 0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 32)                288       \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 9)                 297       \n",
      "=================================================================\n",
      "Total params: 1,641\n",
      "Trainable params: 1,641\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "env = envCube()\n",
    "model = build_model(env.OBSERVATION_SPACE_VALUES, env.ACTION_SPACE_VALUES)\n",
    "print(model.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d0e98e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn = build_agent(model, env.ACTION_SPACE_VALUES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c3e4f6a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 100000 steps ...\n",
      "Interval 1 (0 steps performed)\n",
      " 1001/10000 [==>...........................] - ETA: 4:07 - reward: -1.7403"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "Tensor.op is meaningless when eager execution is enabled.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-fa7255dfb749>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdqn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnb_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvisualize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32me:\\Anaconda3\\envs\\py36\\lib\\site-packages\\rl\\core.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, env, nb_steps, action_repetition, callbacks, verbose, visualize, nb_max_start_steps, start_step_policy, log_interval, nb_max_episode_steps)\u001b[0m\n\u001b[0;32m    192\u001b[0m                     \u001b[1;31m# Force a terminal state.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    193\u001b[0m                     \u001b[0mdone\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 194\u001b[1;33m                 \u001b[0mmetrics\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mterminal\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    195\u001b[0m                 \u001b[0mepisode_reward\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    196\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\Anaconda3\\envs\\py36\\lib\\site-packages\\rl\\agents\\dqn.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, reward, terminal)\u001b[0m\n\u001b[0;32m    323\u001b[0m             \u001b[1;31m# it is still useful to know the actual target to compute metrics properly.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    324\u001b[0m             \u001b[0mins\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mstate0_batch\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mlist\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mstate0_batch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 325\u001b[1;33m             \u001b[0mmetrics\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainable_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtargets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmasks\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mdummy_targets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    326\u001b[0m             \u001b[0mmetrics\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mmetric\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetric\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m  \u001b[1;31m# throw away individual losses\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    327\u001b[0m             \u001b[0mmetrics\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\Anaconda3\\envs\\py36\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[1;34m(self, x, y, sample_weight, class_weight, reset_metrics)\u001b[0m\n\u001b[0;32m   1511\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1512\u001b[0m             \u001b[0mins\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1513\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1514\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\Anaconda3\\envs\\py36\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_make_train_function\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    331\u001b[0m                     \u001b[0mupdates\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mupdates\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mmetrics_updates\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    332\u001b[0m                     \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'train_function'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 333\u001b[1;33m                     **self._function_kwargs)\n\u001b[0m\u001b[0;32m    334\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    335\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_test_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\Anaconda3\\envs\\py36\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36mfunction\u001b[1;34m(inputs, outputs, updates, **kwargs)\u001b[0m\n\u001b[0;32m   3007\u001b[0m     return tf_keras_backend.function(inputs, outputs,\n\u001b[0;32m   3008\u001b[0m                                      \u001b[0mupdates\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mupdates\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3009\u001b[1;33m                                      **kwargs)\n\u001b[0m\u001b[0;32m   3010\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3011\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\Anaconda3\\envs\\py36\\lib\\site-packages\\tensorflow_core\\python\\keras\\backend.py\u001b[0m in \u001b[0;36mfunction\u001b[1;34m(inputs, outputs, updates, name, **kwargs)\u001b[0m\n\u001b[0;32m   3758\u001b[0m       raise ValueError('Session keyword arguments are not support during '\n\u001b[0;32m   3759\u001b[0m                        'eager execution. You passed: %s' % (kwargs,))\n\u001b[1;32m-> 3760\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mEagerExecutionFunction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mupdates\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mupdates\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3761\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3762\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\Anaconda3\\envs\\py36\\lib\\site-packages\\tensorflow_core\\python\\keras\\backend.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, inputs, outputs, updates, name)\u001b[0m\n\u001b[0;32m   3655\u001b[0m             \u001b[0madd_sources\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3656\u001b[0m             \u001b[0mhandle_captures\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3657\u001b[1;33m             base_graph=source_graph)\n\u001b[0m\u001b[0;32m   3658\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3659\u001b[0m         \u001b[0minputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mlifted_map\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\Anaconda3\\envs\\py36\\lib\\site-packages\\tensorflow_core\\python\\eager\\lift_to_graph.py\u001b[0m in \u001b[0;36mlift_to_graph\u001b[1;34m(tensors, graph, sources, disallowed_placeholders, add_sources, handle_captures, base_graph, op_map)\u001b[0m\n\u001b[0;32m    258\u001b[0m         \u001b[0mvisited_ops\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvisited_ops\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    259\u001b[0m         \u001b[0mop_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mop_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 260\u001b[1;33m         add_sources=add_sources))\n\u001b[0m\u001b[0;32m    261\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    262\u001b[0m   \u001b[1;31m# Try to topologically sort the nodes we've extracted. Now we know how many of\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\Anaconda3\\envs\\py36\\lib\\site-packages\\tensorflow_core\\python\\ops\\op_selector.py\u001b[0m in \u001b[0;36mmap_subgraph\u001b[1;34m(init_tensor, sources, disallowed_placeholders, visited_ops, op_outputs, add_sources)\u001b[0m\n\u001b[0;32m    391\u001b[0m       \u001b[0msources\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0madd_sources\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    392\u001b[0m   \"\"\"\n\u001b[1;32m--> 393\u001b[1;33m   \u001b[0mops_to_visit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0m_as_operation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minit_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    394\u001b[0m   \u001b[0mextra_sources\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mobject_identity\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mObjectIdentitySet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    395\u001b[0m   \u001b[1;32mwhile\u001b[0m \u001b[0mops_to_visit\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\Anaconda3\\envs\\py36\\lib\\site-packages\\tensorflow_core\\python\\ops\\op_selector.py\u001b[0m in \u001b[0;36m_as_operation\u001b[1;34m(op_or_tensor)\u001b[0m\n\u001b[0;32m    318\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_as_operation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop_or_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    319\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop_or_tensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 320\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mop_or_tensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mop\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    321\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mop_or_tensor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    322\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\Anaconda3\\envs\\py36\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mop\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1092\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1093\u001b[0m     raise AttributeError(\n\u001b[1;32m-> 1094\u001b[1;33m         \"Tensor.op is meaningless when eager execution is enabled.\")\n\u001b[0m\u001b[0;32m   1095\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1096\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: Tensor.op is meaningless when eager execution is enabled."
     ]
    }
   ],
   "source": [
    "dqn.fit(env, nb_steps=100000, visualize=True, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d9a83a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After training is done, we save the final weights.\n",
    "dqn.save_weights('dqn_n1_weights.h5f', overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fd8c580",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally, evaluate our algorithm for 5 episodes.\n",
    "scores = dqn.test(env, nb_episodes=10, visualize=True)\n",
    "print(np.mean(scores.history['episode_reward']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "663a5cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 删除已生成的\n",
    "del dqn, model\n",
    "\n",
    "# 从新实例化一个dqn和model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd435e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载参数\n",
    "dqn.load_weights('dqn_n1_weights.h5f')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e0c59c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py36",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
